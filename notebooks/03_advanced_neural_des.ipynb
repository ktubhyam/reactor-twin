{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Neural Differential Equations\n",
    "\n",
    "ReactorTwin provides **5 Neural DE variants** for modeling chemical reactor dynamics:\n",
    "\n",
    "1. **Neural ODE** - Standard neural ordinary differential equation\n",
    "2. **Augmented Neural ODE** - Extended state space for more expressive dynamics\n",
    "3. **Latent Neural ODE** - VAE-style encoder-decoder with ODE in latent space\n",
    "4. **Neural SDE** - Stochastic differential equations with built-in uncertainty\n",
    "5. **Neural CDE** - Controlled differential equations for irregular time series\n",
    "\n",
    "This notebook demonstrates each variant on the same CSTR reference data, comparing their expressiveness, parameter counts, and training behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from reactor_twin import ArrheniusKinetics, CSTRReactor, NeuralODE\n",
    "from reactor_twin.core import AugmentedNeuralODE, LatentNeuralODE, NeuralSDE, NeuralCDE\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Reference Data\n",
    "\n",
    "We create a simple CSTR dataset (A -> B reaction) that all five Neural DE models will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinetics = ArrheniusKinetics(\n",
    "    name=\"A_to_B\", num_reactions=1,\n",
    "    params={\"k0\": np.array([0.5]), \"Ea\": np.array([0.0]),\n",
    "            \"stoich\": np.array([[-1, 1]]), \"orders\": np.array([[1, 0]])},\n",
    ")\n",
    "reactor = CSTRReactor(\n",
    "    name=\"cstr\", num_species=2,\n",
    "    params={\"V\": 10.0, \"F\": 1.0, \"C_feed\": [1.0, 0.0], \"T_feed\": 350.0},\n",
    "    kinetics=kinetics, isothermal=True,\n",
    ")\n",
    "\n",
    "y0 = reactor.get_initial_state()\n",
    "t_eval = np.linspace(0, 10, 50)\n",
    "sol = solve_ivp(reactor.ode_rhs, [0, 10], y0, t_eval=t_eval, method=\"LSODA\")\n",
    "\n",
    "z0 = torch.tensor(y0, dtype=torch.float32).unsqueeze(0)\n",
    "t_span = torch.tensor(t_eval, dtype=torch.float32)\n",
    "targets = torch.tensor(sol.y.T, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "print(f\"Data: z0={z0.shape}, t={t_span.shape}, targets={targets.shape}\")\n",
    "plt.plot(t_eval, sol.y[0], 'b-', label='C_A')\n",
    "plt.plot(t_eval, sol.y[1], 'r-', label='C_B')\n",
    "plt.xlabel('Time'); plt.ylabel('Concentration'); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.title('Reference CSTR Trajectory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standard Neural ODE\n",
    "\n",
    "The baseline Neural ODE models dynamics as `dy/dt = f_\\theta(t, y)` where `f_\\theta` is a simple MLP. This is the most straightforward variant and works well for smooth, deterministic systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, z0, t_span, targets, num_epochs=200, lr=1e-3):\n",
    "    \"\"\"Train any Neural DE model and return losses.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(z0, t_span)\n",
    "        loss_dict = model.compute_loss(preds, targets)\n",
    "        loss_dict[\"total\"].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        losses.append(loss_dict[\"total\"].item())\n",
    "    return losses\n",
    "\n",
    "model_ode = NeuralODE(state_dim=2, hidden_dim=32, num_layers=2, solver=\"rk4\", adjoint=False)\n",
    "losses_ode = train_model(model_ode, z0, t_span, targets)\n",
    "print(f\"Neural ODE - Final loss: {losses_ode[-1]:.6f}, Params: {sum(p.numel() for p in model_ode.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Augmented Neural ODE\n",
    "\n",
    "The Augmented Neural ODE extends the state space with extra dimensions, allowing the model to learn more expressive dynamics. This is particularly useful when the true dynamics require crossing trajectories in the original state space, which standard Neural ODEs cannot represent due to the uniqueness theorem of ODEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug = AugmentedNeuralODE(\n",
    "    state_dim=2, augment_dim=4,  # 4 extra dimensions\n",
    "    hidden_dim=32, num_layers=2, solver=\"rk4\", adjoint=False,\n",
    ")\n",
    "losses_aug = train_model(model_aug, z0, t_span, targets)\n",
    "print(f\"Augmented ODE - Final loss: {losses_aug[-1]:.6f}, Params: {sum(p.numel() for p in model_aug.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Latent Neural ODE\n",
    "\n",
    "The Latent Neural ODE uses an encoder-decoder architecture:\n",
    "1. **Encoder**: Maps observations into a latent space (VAE-style with reparameterization)\n",
    "2. **ODE**: Evolves the latent state forward in time\n",
    "3. **Decoder**: Maps latent trajectories back to observation space\n",
    "\n",
    "This is especially powerful for high-dimensional systems where the intrinsic dynamics are low-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_latent = LatentNeuralODE(\n",
    "    state_dim=2, latent_dim=4,\n",
    "    encoder_hidden_dim=32, decoder_hidden_dim=32,\n",
    "    encoder_type=\"mlp\",\n",
    "    hidden_dim=32, num_layers=2,\n",
    "    solver=\"rk4\", adjoint=False,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model_latent.parameters(), lr=1e-3)\n",
    "losses_latent = []\n",
    "model_latent.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    preds = model_latent(z0, t_span)\n",
    "    loss_dict = model_latent.compute_loss(preds, targets)\n",
    "    loss_dict[\"total\"].backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_latent.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    losses_latent.append(loss_dict[\"total\"].item())\n",
    "\n",
    "print(f\"Latent ODE - Final loss: {losses_latent[-1]:.6f}, Params: {sum(p.numel() for p in model_latent.parameters()):,}\")\n",
    "\n",
    "# Examine latent space\n",
    "model_latent.eval()\n",
    "with torch.no_grad():\n",
    "    z_mean, z_logvar = model_latent.encode(z0)\n",
    "print(f\"Latent mean: {z_mean[0].numpy()}\")\n",
    "print(f\"Latent logvar: {z_logvar[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural SDE\n",
    "\n",
    "The Neural SDE models stochastic dynamics: `dy = f_\\theta(t,y)dt + g_\\phi(t,y)dW`, where `f` is the drift and `g` is the diffusion. This provides **built-in uncertainty quantification** since each forward pass samples a different Brownian path.\n",
    "\n",
    "**Note:** Requires the `torchsde` package (`pip install torchsde`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_sde = NeuralSDE(\n",
    "        state_dim=2, hidden_dim=32, num_layers=2,\n",
    "        noise_type=\"diagonal\", sde_type=\"ito\",\n",
    "        solver=\"euler\", adjoint=False,\n",
    "    )\n",
    "    losses_sde = train_model(model_sde, z0, t_span, targets, num_epochs=100)\n",
    "    print(f\"Neural SDE - Final loss: {losses_sde[-1]:.6f}\")\n",
    "    \n",
    "    # Multiple forward passes give different results (stochastic)\n",
    "    model_sde.eval()\n",
    "    sde_preds = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            pred = model_sde(z0, t_span)\n",
    "            sde_preds.append(pred[0].numpy())\n",
    "    sde_preds = np.array(sde_preds)\n",
    "    print(f\"Prediction spread (std): {sde_preds.std(axis=0).mean():.6f}\")\n",
    "except ImportError:\n",
    "    print(\"torchsde not installed - skipping Neural SDE demo\")\n",
    "    print(\"Install with: pip install torchsde\")\n",
    "    losses_sde = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural CDE\n",
    "\n",
    "Neural Controlled Differential Equations are designed for **irregular time series**. Instead of a fixed ODE, the dynamics are driven by a continuous control signal (interpolated from observations):\n",
    "\n",
    "`dy = f_\\theta(y) dX/dt dt`\n",
    "\n",
    "This makes them naturally suited to data with missing values or non-uniform sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_cde = NeuralCDE(\n",
    "        state_dim=2, hidden_dim=32, num_layers=2,\n",
    "        solver=\"rk4\", adjoint=False,\n",
    "    )\n",
    "    losses_cde = train_model(model_cde, z0, t_span, targets, num_epochs=100)\n",
    "    print(f\"Neural CDE - Final loss: {losses_cde[-1]:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Neural CDE requires torchcde: {e}\")\n",
    "    losses_cde = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Let's compare all trained models on training loss convergence and prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].semilogy(losses_ode, label='Neural ODE')\n",
    "axes[0].semilogy(losses_aug, label='Augmented ODE')\n",
    "axes[0].semilogy(losses_latent, label='Latent ODE')\n",
    "if losses_sde: axes[0].semilogy(losses_sde, label='Neural SDE')\n",
    "if losses_cde: axes[0].semilogy(losses_cde, label='Neural CDE')\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Comparison'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions\n",
    "true_np = targets[0].numpy()\n",
    "axes[1].plot(t_eval, true_np[:, 0], 'k--', linewidth=2, label='True C_A')\n",
    "\n",
    "for model, name in [(model_ode, 'Neural ODE'), (model_aug, 'Augmented'), (model_latent, 'Latent')]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(z0, t_span)\n",
    "    axes[1].plot(t_eval, pred[0, :, 0].numpy(), label=name)\n",
    "\n",
    "axes[1].set_xlabel('Time'); axes[1].set_ylabel('C_A')\n",
    "axes[1].set_title('C_A Predictions'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Variant | Key Feature | Best For |\n",
    "|---------|-------------|----------|\n",
    "| **Neural ODE** | Simple, efficient | Smooth deterministic dynamics |\n",
    "| **Augmented Neural ODE** | Extended state space | Complex dynamics with crossing trajectories |\n",
    "| **Latent Neural ODE** | Encoder-decoder + latent ODE | High-dimensional systems with low-dimensional intrinsic dynamics |\n",
    "| **Neural SDE** | Stochastic diffusion term | Systems with inherent noise, uncertainty quantification |\n",
    "| **Neural CDE** | Control-driven dynamics | Irregular time series, missing data |\n",
    "\n",
    "All variants share the same training interface (`compute_loss`, `forward`) and can be used interchangeably in the digital twin pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}