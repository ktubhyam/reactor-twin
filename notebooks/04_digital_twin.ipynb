{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Twin: Monitoring, Detection, and Control\n",
    "\n",
    "The ReactorTwin digital twin layer adds a complete operational stack on top of Neural DE models:\n",
    "\n",
    "- **EKF State Estimation** - Fuse model predictions with noisy measurements\n",
    "- **Fault Detection** - Statistical Process Control (SPC) with EWMA and CUSUM charts, plus residual-based detection\n",
    "- **MPC Control** - Model Predictive Control using the Neural ODE as a plant model\n",
    "- **Online Adaptation** - Continual learning with Elastic Weight Consolidation (EWC)\n",
    "- **Meta-Learning** - Fast adaptation to new operating conditions\n",
    "\n",
    "This notebook demonstrates the full pipeline from training a plant model to deploying it as a digital twin with monitoring, fault detection, and optimal control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from reactor_twin import (\n",
    "    ArrheniusKinetics, CSTRReactor, NeuralODE,\n",
    "    EKFStateEstimator, FaultDetector, MPCController, OnlineAdapter,\n",
    ")\n",
    "from reactor_twin.digital_twin import SPCChart\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Train a NeuralODE Plant Model\n",
    "\n",
    "Before we can build a digital twin, we need a trained Neural ODE that captures the plant dynamics. We use a simple CSTR with an A -> B reaction as our reference system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinetics = ArrheniusKinetics(\n",
    "    name=\"A_to_B\", num_reactions=1,\n",
    "    params={\"k0\": np.array([0.5]), \"Ea\": np.array([0.0]),\n",
    "            \"stoich\": np.array([[-1, 1]]), \"orders\": np.array([[1, 0]])},\n",
    ")\n",
    "reactor = CSTRReactor(\n",
    "    name=\"cstr\", num_species=2,\n",
    "    params={\"V\": 10.0, \"F\": 1.0, \"C_feed\": [1.0, 0.0], \"T_feed\": 350.0,\n",
    "            \"C_initial\": [0.8, 0.2]},\n",
    "    kinetics=kinetics, isothermal=True,\n",
    ")\n",
    "\n",
    "state_dim = 2\n",
    "y0 = reactor.get_initial_state()\n",
    "dt = 0.1\n",
    "t_eval = np.linspace(0, 10, 101)\n",
    "sol = solve_ivp(reactor.ode_rhs, [0, 10], y0, t_eval=t_eval, method=\"LSODA\")\n",
    "true_states = sol.y.T\n",
    "\n",
    "# Train NeuralODE\n",
    "model = NeuralODE(state_dim=2, hidden_dim=32, num_layers=2, solver=\"rk4\", adjoint=False)\n",
    "z0_train = torch.tensor(y0, dtype=torch.float32).unsqueeze(0)\n",
    "t_train = torch.tensor(t_eval[:51], dtype=torch.float32)\n",
    "targets_train = torch.tensor(true_states[:51], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(z0_train, t_train)\n",
    "    loss = model.compute_loss(preds, targets_train)[\"total\"]\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Model trained. Final loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EKF State Estimation\n",
    "\n",
    "The Extended Kalman Filter (EKF) fuses the Neural ODE's predictions with noisy measurements to produce optimal state estimates. It uses the Neural ODE as the process model and linearizes it at each step for the Kalman update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noisy measurements\n",
    "noise_std = 0.05\n",
    "measurements_np = true_states[1:] + noise_std * np.random.randn(100, 2)\n",
    "measurements_np = np.maximum(measurements_np, 0)\n",
    "measurements = torch.tensor(measurements_np, dtype=torch.float32)\n",
    "\n",
    "# Set up EKF\n",
    "ekf = EKFStateEstimator(\n",
    "    model=model, state_dim=2,\n",
    "    Q=1e-3, R=noise_std**2, P0=0.1, dt=0.1,\n",
    ")\n",
    "\n",
    "z0_ekf = torch.tensor(y0, dtype=torch.float32)\n",
    "t_ekf = torch.tensor(t_eval[1:], dtype=torch.float32)\n",
    "result = ekf.filter(measurements=measurements, z0=z0_ekf, t_span=t_ekf)\n",
    "\n",
    "filtered = result[\"states\"].numpy()\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for i, name in enumerate(['C_A', 'C_B']):\n",
    "    axes[i].plot(t_eval[1:], true_states[1:, i], 'k-', linewidth=2, label='True')\n",
    "    axes[i].plot(t_eval[1:], measurements_np[:, i], '.', alpha=0.3, label='Measured')\n",
    "    axes[i].plot(t_eval[1:], filtered[:, i], 'r-', linewidth=1.5, label='EKF Filtered')\n",
    "    axes[i].set_xlabel('Time'); axes[i].set_ylabel(name)\n",
    "    axes[i].set_title(f'EKF State Estimation: {name}')\n",
    "    axes[i].legend(); axes[i].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "rmse_meas = np.sqrt(np.mean((measurements_np - true_states[1:])**2))\n",
    "rmse_ekf = np.sqrt(np.mean((filtered - true_states[1:])**2))\n",
    "print(f\"RMSE (measurements): {rmse_meas:.4f}\")\n",
    "print(f\"RMSE (EKF filtered): {rmse_ekf:.4f}\")\n",
    "print(f\"Improvement: {rmse_meas/rmse_ekf:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fault Detection with SPC Charts\n",
    "\n",
    "Statistical Process Control (SPC) detects anomalies using two complementary methods:\n",
    "- **EWMA** (Exponentially Weighted Moving Average) - Sensitive to small, sustained shifts\n",
    "- **CUSUM** (Cumulative Sum) - Optimal for detecting mean shifts of known magnitude\n",
    "\n",
    "We first establish a baseline from normal operation, then monitor for a step change fault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate normal operation data\n",
    "normal_data = np.column_stack([\n",
    "    0.5 + 0.02 * np.random.randn(200),\n",
    "    0.5 + 0.02 * np.random.randn(200),\n",
    "])\n",
    "\n",
    "# Set up SPC\n",
    "spc = SPCChart(num_vars=2, ewma_lambda=0.2, ewma_L=3.0, cusum_k=0.5, cusum_h=5.0)\n",
    "spc.set_baseline(normal_data)\n",
    "\n",
    "# Test data with fault at sample 50\n",
    "test_data = np.column_stack([\n",
    "    0.5 + 0.02 * np.random.randn(100),\n",
    "    0.5 + 0.02 * np.random.randn(100),\n",
    "])\n",
    "test_data[50:, 0] += 0.1  # Step change in C_A\n",
    "\n",
    "# Monitor\n",
    "ewma_alarms = []\n",
    "cusum_alarms = []\n",
    "for i in range(100):\n",
    "    result = spc.update(test_data[i])\n",
    "    ewma_alarms.append(result[\"ewma_alarm\"].any())\n",
    "    cusum_alarms.append(result[\"cusum_alarm\"].any())\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "axes[0].plot(test_data[:, 0], 'b-', label='C_A')\n",
    "axes[0].axvline(x=50, color='r', linestyle='--', label='Fault onset')\n",
    "axes[0].axhline(y=0.5, color='k', linestyle=':', alpha=0.5)\n",
    "axes[0].set_ylabel('C_A'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_title('Fault Detection: Step Change in C_A')\n",
    "\n",
    "alarm_timeline = np.array([1 if e else 0 for e in ewma_alarms])\n",
    "cusum_timeline = np.array([1 if c else 0 for c in cusum_alarms])\n",
    "axes[1].fill_between(range(100), alarm_timeline, alpha=0.3, label='EWMA Alarm', color='orange')\n",
    "axes[1].fill_between(range(100), cusum_timeline * 0.5, alpha=0.3, label='CUSUM Alarm', color='red')\n",
    "axes[1].axvline(x=50, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Sample'); axes[1].set_ylabel('Alarm')\n",
    "axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "first_ewma = next((i for i, a in enumerate(ewma_alarms) if a), None)\n",
    "first_cusum = next((i for i, a in enumerate(cusum_alarms) if a), None)\n",
    "print(f\"First EWMA alarm: sample {first_ewma} (delay: {first_ewma - 50 if first_ewma else 'N/A'})\")\n",
    "print(f\"First CUSUM alarm: sample {first_cusum} (delay: {first_cusum - 50 if first_cusum else 'N/A'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Predictive Control (MPC)\n",
    "\n",
    "MPC uses the Neural ODE as a plant model to compute optimal control actions over a receding horizon. At each step, it solves an optimization problem to find the control sequence that minimizes the tracking error while respecting constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with control input\n",
    "model_ctrl = NeuralODE(\n",
    "    state_dim=2, input_dim=1, hidden_dim=32, num_layers=2,\n",
    "    solver=\"rk4\", adjoint=False,\n",
    ")\n",
    "\n",
    "# Train with Euler rollout (MPC uses ode_func directly)\n",
    "targets_ctrl = torch.tensor(sol.y.T, dtype=torch.float32)\n",
    "dt_train = t_eval[1] - t_eval[0]\n",
    "optimizer = torch.optim.Adam(model_ctrl.parameters(), lr=1e-3)\n",
    "model_ctrl.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    z = torch.tensor(y0, dtype=torch.float32).unsqueeze(0)\n",
    "    u_zero = torch.zeros(1, 1)\n",
    "    total_loss = torch.tensor(0.0)\n",
    "    for k in range(20):\n",
    "        dzdt = model_ctrl.ode_func(torch.tensor(0.0), z, u_zero)\n",
    "        z = z + dzdt * dt_train\n",
    "        total_loss = total_loss + torch.mean((z - targets_ctrl[k+1].unsqueeze(0))**2)\n",
    "    (total_loss / 20).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_ctrl.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "# Set up MPC\n",
    "mpc = MPCController(model=model_ctrl, horizon=5, dt=0.2, max_iter=10)\n",
    "\n",
    "# Run MPC loop\n",
    "y_ref = torch.tensor([0.23, 0.77], dtype=torch.float32)\n",
    "z_current = torch.tensor([0.8, 0.2], dtype=torch.float32)\n",
    "\n",
    "states = [z_current.numpy().copy()]\n",
    "controls = []\n",
    "for step in range(15):\n",
    "    u_applied, info = mpc.step(z_current, y_ref)\n",
    "    controls.append(u_applied.item())\n",
    "    z_batch = z_current.unsqueeze(0)\n",
    "    u_batch = u_applied.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        dzdt = model_ctrl.ode_func(torch.tensor(0.0), z_batch, u_batch).squeeze(0)\n",
    "    z_current = z_current + dzdt * 0.2\n",
    "    states.append(z_current.detach().numpy().copy())\n",
    "\n",
    "states = np.array(states)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(states[:, 0], 'b-o', markersize=3, label='C_A')\n",
    "axes[0].plot(states[:, 1], 'r-o', markersize=3, label='C_B')\n",
    "axes[0].axhline(y=y_ref[0].item(), color='b', linestyle='--', alpha=0.5, label='C_A ref')\n",
    "axes[0].axhline(y=y_ref[1].item(), color='r', linestyle='--', alpha=0.5, label='C_B ref')\n",
    "axes[0].set_xlabel('MPC Step'); axes[0].set_ylabel('Concentration')\n",
    "axes[0].set_title('MPC Setpoint Tracking'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].step(range(len(controls)), controls, 'g-', where='post')\n",
    "axes[1].set_xlabel('MPC Step'); axes[1].set_ylabel('Control Input')\n",
    "axes[1].set_title('Control Actions'); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial error: {np.sqrt(np.mean((states[0] - y_ref.numpy())**2)):.4f}\")\n",
    "print(f\"Final error:   {np.sqrt(np.mean((states[-1] - y_ref.numpy())**2)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Online Adaptation\n",
    "\n",
    "When the real plant drifts from the model's training distribution, online adaptation updates the Neural ODE parameters using new data. **Elastic Weight Consolidation (EWC)** prevents catastrophic forgetting by penalizing changes to parameters that were important for previously learned tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = OnlineAdapter(\n",
    "    model=model, buffer_size=100, ewc_lambda=100.0, lr=1e-4,\n",
    ")\n",
    "\n",
    "# Simulate new data arriving (slight distribution shift)\n",
    "new_y0 = np.array([0.9, 0.1])\n",
    "new_t = np.linspace(0, 5, 30)\n",
    "new_sol = solve_ivp(reactor.ode_rhs, [0, 5], new_y0, t_eval=new_t, method=\"LSODA\")\n",
    "\n",
    "z0_new = torch.tensor(new_y0, dtype=torch.float32)\n",
    "t_new = torch.tensor(new_t, dtype=torch.float32)\n",
    "targets_new = torch.tensor(new_sol.y.T, dtype=torch.float32)\n",
    "\n",
    "# Add experience and adapt\n",
    "adapter.add_experience(z0_new, t_new, targets_new)\n",
    "losses = adapter.adapt(num_steps=20)\n",
    "\n",
    "print(f\"Adaptation losses: {losses[0]:.6f} -> {losses[-1]:.6f}\")\n",
    "print(f\"EWC prevents forgetting while learning new patterns\")\n",
    "\n",
    "# Consolidate for next round\n",
    "adapter.consolidate()\n",
    "print(\"Parameters consolidated for next EWC round\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The digital twin layer provides a complete monitoring and control stack on top of the Neural ODE models:\n",
    "\n",
    "| Component | Purpose | Key Feature |\n",
    "|-----------|---------|-------------|\n",
    "| **EKF State Estimator** | Fuse predictions with measurements | Optimal state estimates from noisy sensors |\n",
    "| **SPC Charts** | Detect process anomalies | EWMA + CUSUM for different fault types |\n",
    "| **MPC Controller** | Optimal control actions | Receding-horizon optimization using Neural ODE |\n",
    "| **Online Adapter** | Continual learning | EWC prevents catastrophic forgetting |\n",
    "\n",
    "Together, these components enable a Neural ODE to serve as a living digital twin that tracks the real plant, detects faults early, controls the process optimally, and adapts to changing conditions over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}